{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "All necessary imports"
      ],
      "metadata": {
        "id": "09840gEc0_TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pygame\n",
        "import sys\n",
        "import random\n",
        "import cv2\n",
        "from collections import deque\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0cpsKQPS0-_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3552ce60-d771-428e-e5a2-41e49b92d770"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing down the game, specifying the entities that will take place and defining the reward system"
      ],
      "metadata": {
        "id": "Bdr6bjH103XX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c1bMmztM0vNx"
      },
      "outputs": [],
      "source": [
        "player_moves = {\n",
        "    'L': np.array([-1.,0.]),\n",
        "    'R': np.array([1.,0.]),\n",
        "    'U': np.array([0.,-1.]),\n",
        "    'D': np.array([0.,1.])\n",
        "}\n",
        "initial_playersize = 4\n",
        "\n",
        "class snakeclass(object):\n",
        "    def __init__(self, gridsize):\n",
        "        self.pos = np.array([gridsize//2, gridsize//2]).astype('float')\n",
        "        self.dir = np.array([1.,0.])\n",
        "        self.len = initial_playersize\n",
        "        self.prevpos = [np.array([gridsize//2, gridsize//2]).astype('float')]\n",
        "        self.gridsize = gridsize\n",
        "\n",
        "    def move(self):\n",
        "        self.pos += self.dir\n",
        "        self.prevpos.append(self.pos.copy())\n",
        "        self.prevpos = self.prevpos[-self.len-1:]\n",
        "\n",
        "    def checkdead(self, pos):\n",
        "        if pos[0] <= -1 or pos[0] >= self.gridsize:\n",
        "            return True\n",
        "        elif pos[1] <= -1 or pos[1] >= self.gridsize:\n",
        "            return True\n",
        "        elif list(pos) in [list(item) for item in self.prevpos[:-1]]:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def getproximity(self):\n",
        "        L = self.pos - np.array([1,0])\n",
        "        R = self.pos + np.array([1,0])\n",
        "        U = self.pos - np.array([0,1])\n",
        "        D = self.pos + np.array([0,1])\n",
        "        possdirections = [L, R, U, D]\n",
        "        proximity = [int(self.checkdead(x)) for x in possdirections]\n",
        "        return proximity\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len + 1\n",
        "\n",
        "class appleclass(object):\n",
        "    def __init__(self, gridsize):\n",
        "        self.pos = np.random.randint(1,gridsize,2)\n",
        "        self.score = 0\n",
        "        self.gridsize = gridsize\n",
        "\n",
        "    def eaten(self):\n",
        "        self.pos = np.random.randint(1,self.gridsize,2)\n",
        "        self.score += 1\n",
        "\n",
        "class GameEnvironment(object):\n",
        "    def __init__(self, gridsize, nothing, dead, apple):\n",
        "        self.snake = snakeclass(gridsize)\n",
        "        self.apple = appleclass(gridsize)\n",
        "        self.game_over = False\n",
        "        self.gridsize = gridsize\n",
        "        self.reward_nothing = nothing\n",
        "        self.reward_dead = dead\n",
        "        self.reward_apple = apple\n",
        "        self.time_since_apple = 0\n",
        "\n",
        "    def resetgame(self):\n",
        "        self.apple.pos = np.random.randint(1, self.gridsize, 2).astype('float')\n",
        "        self.apple.score = 0\n",
        "        self.snake.pos = np.random.randint(1, self.gridsize, 2).astype('float')\n",
        "        self.snake.prevpos = [self.snake.pos.copy().astype('float')]\n",
        "        self.snake.len = initial_playersize\n",
        "        self.game_over = False\n",
        "\n",
        "    def get_boardstate(self):\n",
        "        return [self.snake.pos, self.snake.dir, self.snake.prevpos, self.apple.pos, self.apple.score, self.game_over]\n",
        "\n",
        "    def update_boardstate(self, move):\n",
        "        reward = self.reward_nothing\n",
        "        Done = False\n",
        "\n",
        "        if move == 0:\n",
        "            if not (self.snake.dir == player_moves['R']).all():\n",
        "                self.snake.dir = player_moves['L']\n",
        "        if move == 1:\n",
        "            if not (self.snake.dir == player_moves['L']).all():\n",
        "                self.snake.dir = player_moves['R']\n",
        "        if move == 2:\n",
        "            if not (self.snake.dir == player_moves['D']).all():\n",
        "                self.snake.dir = player_moves['U']\n",
        "        if move == 3:\n",
        "            if not (self.snake.dir == player_moves['U']).all():\n",
        "                self.snake.dir = player_moves['D']\n",
        "\n",
        "        self.snake.move()\n",
        "        self.time_since_apple += 1\n",
        "\n",
        "        if self.time_since_apple == 100:\n",
        "            self.game_over = True\n",
        "            reward = self.reward_dead\n",
        "            self.time_since_apple = 0\n",
        "\n",
        "\n",
        "        if self.snake.checkdead(self.snake.pos) == True:\n",
        "            self.game_over = True\n",
        "            reward = self.reward_dead\n",
        "            self.time_since_apple = 0\n",
        "            Done = True\n",
        "\n",
        "        elif (self.snake.pos == self.apple.pos).all():\n",
        "            self.apple.eaten()\n",
        "            self.snake.len += 1\n",
        "            self.time_since_apple = 0\n",
        "            reward = self.reward_apple\n",
        "\n",
        "        len_of_snake = len(self.snake)\n",
        "\n",
        "        return reward, Done, len_of_snake"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model: a feedforward network used as a Q-network.\n",
        "The input tensor is composed of:\n",
        "\n",
        "*   apple.pos: Position of the apple.\n",
        "*   player.dir: Direction in which the snake is facing.\n",
        "\n",
        "*   proximity: Proximity pixels around the snake.\n",
        "*   player.pos: Position of the snake.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_dMoh-LJ5W2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        l1 = self.relu(self.fc1(x.float()))\n",
        "        l2 = self.relu(self.fc2(l1))\n",
        "        l3 = self.relu(self.fc3(l2))\n",
        "        l4 = self.fc4(l3)\n",
        "        return l4\n",
        "\n",
        "def get_network_input(player, apple):\n",
        "    proximity = player.getproximity()\n",
        "    x = torch.cat([torch.from_numpy(player.pos).double(), torch.from_numpy(apple.pos).double(),\n",
        "                   torch.from_numpy(player.dir).double(), torch.tensor(proximity).double()])\n",
        "    return x"
      ],
      "metadata": {
        "id": "ANjtpW6j5XHO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storing and managing transitions"
      ],
      "metadata": {
        "id": "MAzBG2UY-ztP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "    def __init__(self, max_size):\n",
        "        self.max_size = max_size\n",
        "        self.buffer = []\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state_batch = []\n",
        "        action_batch = []\n",
        "        reward_batch = []\n",
        "        next_state_batch = []\n",
        "        done_batch = []\n",
        "\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        for experience in batch:\n",
        "            state, action, reward, next_state, done = experience\n",
        "            state_batch.append(state)\n",
        "            action_batch.append(action)\n",
        "            reward_batch.append(reward)\n",
        "            next_state_batch.append(next_state)\n",
        "            done_batch.append(done)\n",
        "\n",
        "        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "    def truncate(self):\n",
        "        self.buffer = self.buffer[-self.max_size:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "JUqvPy8H-0B9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training"
      ],
      "metadata": {
        "id": "ui6BWE27-zGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/checkpoints/'\n",
        "\n",
        "model = QNetwork(input_dim=10, hidden_dim=20, output_dim=5)\n",
        "epsilon = 0.1\n",
        "gridsize = 15\n",
        "GAMMA = 0.9\n",
        "\n",
        "board = GameEnvironment(gridsize, nothing=0, dead=-1, apple=1)\n",
        "memory = ReplayMemory(1000)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
        "\n",
        "def run_episode(num_games):\n",
        "    run = True\n",
        "    move=0\n",
        "    games_played = 0\n",
        "    total_reward = 0\n",
        "    episode_games = 0\n",
        "    len_array = []\n",
        "\n",
        "    while run:\n",
        "        state = get_network_input(board.snake, board.apple)\n",
        "        action_0 = model(state)\n",
        "        rand = np.random.uniform(0,1)\n",
        "        if rand > epsilon:\n",
        "            action = torch.argmax(action_0)\n",
        "        else:\n",
        "            action = np.random.randint(0,5)\n",
        "\n",
        "        reward, done, len_of_snake = board.update_boardstate(action)\n",
        "        next_state = get_network_input(board.snake, board.apple)\n",
        "\n",
        "        memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        episode_games += 1\n",
        "\n",
        "        if board.game_over == True:\n",
        "            games_played += 1\n",
        "            len_array.append(len_of_snake)\n",
        "            board.resetgame()\n",
        "\n",
        "            if num_games == games_played:\n",
        "                run = False\n",
        "\n",
        "    avg_len_of_snake = np.mean(len_array)\n",
        "    max_len_of_snake = np.max(len_array)\n",
        "    return total_reward, avg_len_of_snake, max_len_of_snake\n",
        "MSE = nn.MSELoss()\n",
        "def learn(num_updates, batch_size):\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(num_updates):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        sample = memory.sample(batch_size)\n",
        "\n",
        "        states, actions, rewards, next_states, dones = sample\n",
        "        states = torch.cat([x.unsqueeze(0) for x in states], dim=0)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.cat([x.unsqueeze(0) for x in next_states])\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        q_local = model.forward(states)\n",
        "        next_q_value = model.forward(next_states)\n",
        "\n",
        "        Q_expected  = q_local.gather(1, actions.unsqueeze(0).transpose(0,1)).transpose(0,1).squeeze(0)\n",
        "\n",
        "        Q_targets_next  = torch.max(next_q_value, 1)[0]*(torch.ones(dones.size()) - dones)\n",
        "\n",
        "        Q_targets  = rewards + GAMMA * Q_targets_next\n",
        "\n",
        "        loss = MSE(Q_expected, Q_targets)\n",
        "\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss\n",
        "num_episodes = 60000\n",
        "num_updates = 500\n",
        "print_every = 10\n",
        "games_in_episode = 30\n",
        "batch_size = 20\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores_array = []\n",
        "    avg_scores_array = []\n",
        "\n",
        "    avg_len_array = []\n",
        "    avg_max_len_array = []\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "\n",
        "    for i_episode in range(num_episodes+1):\n",
        "\n",
        "        score, avg_len, max_len = run_episode(games_in_episode)\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        scores_array.append(score)\n",
        "        avg_len_array.append(avg_len)\n",
        "        avg_max_len_array.append(max_len)\n",
        "\n",
        "\n",
        "        avg_score = np.mean(scores_deque)\n",
        "        avg_scores_array.append(avg_score)\n",
        "\n",
        "        total_loss = learn(num_updates, batch_size)\n",
        "\n",
        "        dt = (int)(time.time() - time_start)\n",
        "\n",
        "        if i_episode % print_every == 0 and i_episode > 0:\n",
        "            print('Ep.: {:6}, Loss: {:.3f}, Avg.Score: {:.2f}, Avg.LenOfSnake: {:.2f}, Max.LenOfSnake:  {:.2f} Time: {:02}:{:02}:{:02} '.\\\n",
        "                  format(i_episode, total_loss, score, avg_len, max_len, dt//3600, dt%3600//60, dt%60))\n",
        "\n",
        "        memory.truncate()\n",
        "\n",
        "        if i_episode % 250 == 0 and i_episode > 0:\n",
        "            torch.save(model.state_dict(), save_path + 'Snake_{}'.format(i_episode))\n",
        "\n",
        "\n",
        "    return scores_array, avg_scores_array, avg_len_array, avg_max_len_array\n",
        "\n",
        "scores, avg_scores, avg_len_of_snake, max_len_of_snake = train()"
      ],
      "metadata": {
        "id": "ldwEZoI0-za0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d478a1b-ee9b-4ccb-c3e9-e4626b4baabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep.:     10, Loss: 30.767, Avg.Score: -29.00, Avg.LenOfSnake: 5.03, Max.LenOfSnake:  6.00 Time: 00:00:18 \n",
            "Ep.:     20, Loss: 11.130, Avg.Score: -27.00, Avg.LenOfSnake: 5.10, Max.LenOfSnake:  6.00 Time: 00:00:34 \n",
            "Ep.:     30, Loss: 12.436, Avg.Score: -27.00, Avg.LenOfSnake: 5.10, Max.LenOfSnake:  6.00 Time: 00:00:52 \n",
            "Ep.:     40, Loss: 7.188, Avg.Score: -30.00, Avg.LenOfSnake: 5.00, Max.LenOfSnake:  5.00 Time: 00:01:08 \n",
            "Ep.:     50, Loss: 5.708, Avg.Score: -29.00, Avg.LenOfSnake: 5.03, Max.LenOfSnake:  6.00 Time: 00:01:25 \n",
            "Ep.:     60, Loss: 6.213, Avg.Score: -28.00, Avg.LenOfSnake: 5.07, Max.LenOfSnake:  6.00 Time: 00:01:42 \n",
            "Ep.:     70, Loss: 7.099, Avg.Score: -28.00, Avg.LenOfSnake: 5.07, Max.LenOfSnake:  6.00 Time: 00:01:58 \n",
            "Ep.:     80, Loss: 6.804, Avg.Score: -27.00, Avg.LenOfSnake: 5.10, Max.LenOfSnake:  7.00 Time: 00:02:16 \n",
            "Ep.:     90, Loss: 5.390, Avg.Score: -28.00, Avg.LenOfSnake: 5.07, Max.LenOfSnake:  6.00 Time: 00:02:33 \n",
            "Ep.:    100, Loss: 4.781, Avg.Score: -30.00, Avg.LenOfSnake: 5.00, Max.LenOfSnake:  5.00 Time: 00:02:50 \n",
            "Ep.:    110, Loss: 6.460, Avg.Score: -26.00, Avg.LenOfSnake: 5.13, Max.LenOfSnake:  6.00 Time: 00:03:08 \n",
            "Ep.:    120, Loss: 6.815, Avg.Score: -26.00, Avg.LenOfSnake: 5.13, Max.LenOfSnake:  7.00 Time: 00:03:26 \n",
            "Ep.:    130, Loss: 5.578, Avg.Score: -28.00, Avg.LenOfSnake: 5.07, Max.LenOfSnake:  6.00 Time: 00:03:45 \n",
            "Ep.:    140, Loss: 6.805, Avg.Score: -25.00, Avg.LenOfSnake: 5.17, Max.LenOfSnake:  7.00 Time: 00:04:04 \n",
            "Ep.:    150, Loss: 6.394, Avg.Score: -27.00, Avg.LenOfSnake: 5.10, Max.LenOfSnake:  6.00 Time: 00:04:23 \n",
            "Ep.:    160, Loss: 5.356, Avg.Score: -24.00, Avg.LenOfSnake: 5.20, Max.LenOfSnake:  6.00 Time: 00:04:42 \n",
            "Ep.:    170, Loss: 5.790, Avg.Score: -23.00, Avg.LenOfSnake: 5.23, Max.LenOfSnake:  7.00 Time: 00:05:01 \n",
            "Ep.:    180, Loss: 6.111, Avg.Score: -27.00, Avg.LenOfSnake: 5.10, Max.LenOfSnake:  6.00 Time: 00:05:23 \n",
            "Ep.:    190, Loss: 4.945, Avg.Score: -26.00, Avg.LenOfSnake: 5.13, Max.LenOfSnake:  7.00 Time: 00:05:42 \n",
            "Ep.:    200, Loss: 6.205, Avg.Score: -26.00, Avg.LenOfSnake: 5.13, Max.LenOfSnake:  6.00 Time: 00:06:01 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance visualization"
      ],
      "metadata": {
        "id": "Eyiq5fP9L7Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "print('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\n",
        "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg score on 100 episodes\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episodes #')\n",
        "plt.show()\n",
        "ax1 = fig.add_subplot(121)\n",
        "plt.plot(np.arange(1, len(avg_len_of_snake)+1), avg_len_of_snake, label=\"Avg Len of Snake\")\n",
        "plt.plot(np.arange(1, len(max_len_of_snake)+1), max_len_of_snake, label=\"Max Len of Snake\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "plt.ylabel('Length of Snake')\n",
        "plt.xlabel('Episodes #')\n",
        "plt.show()\n",
        "n, bins, patches = plt.hist(max_len_of_snake, 45, density=1, facecolor='green', alpha=0.75)\n",
        "l = plt.plot(np.arange(1, len(bins) + 1), 'r--', linewidth=1)\n",
        "mu = round(np.mean(max_len_of_snake), 2)\n",
        "sigma = round(np.std(max_len_of_snake), 2)\n",
        "median = round(np.median(max_len_of_snake), 2)\n",
        "print('mu: ', mu, ', sigma: ', sigma, ', median: ', median)\n",
        "plt.xlabel('Max.Lengths, mu = {:.2f}, sigma={:.2f},  median: {:.2f}'.format(mu, sigma, median))\n",
        "plt.ylabel('Probability')\n",
        "plt.title('Histogram of Max.Lengths')\n",
        "plt.axis([4, 44, 0, 0.15])\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bS4pwae4L99M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the watch agent with a video visualization of the played game"
      ],
      "metadata": {
        "id": "KDMvStFF7vVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gridsize = 23\n",
        "framerate = 10\n",
        "block_size = 30\n",
        "\n",
        "snake_name = 'Snake_59000'\n",
        "\n",
        "model = QNetwork(input_dim=10, hidden_dim=20, output_dim=5)\n",
        "model.load_state_dict(torch.load('./dir_chk_lr0.00001/' + snake_name))\n",
        "\n",
        "board = GameEnvironment(gridsize, nothing=0, dead=-1, apple=1)\n",
        "windowwidth = gridsize*block_size*2\n",
        "windowheight = gridsize*block_size\n",
        "\n",
        "pygame.init()\n",
        "win = pygame.display.set_mode((windowwidth, windowheight))\n",
        "pygame.display.set_caption(\"snake\")\n",
        "font = pygame.font.SysFont('Helvetica', 14)\n",
        "clock = pygame.time.Clock()\n",
        "\n",
        "\n",
        "VIDEO = []\n",
        "\n",
        "def drawboard(snake, apple):\n",
        "    win.fill((0,0,0))\n",
        "    for pos in snake.prevpos:\n",
        "        pygame.draw.rect(win, (0,255,0), (pos[0]*block_size, pos[1]*block_size, block_size, block_size))\n",
        "    pygame.draw.rect(win, (255, 0, 0), (apple.pos[0]*block_size, apple.pos[1]*block_size, block_size, block_size))\n",
        "\n",
        "runGame = True\n",
        "\n",
        "prev_len_of_snake = 0\n",
        "\n",
        "while runGame:\n",
        "    clock.tick(framerate)\n",
        "\n",
        "    state_0 = get_network_input(board.snake, board.apple)\n",
        "    state = model(state_0)\n",
        "\n",
        "    action = torch.argmax(state)\n",
        "\n",
        "    reward, done, len_of_snake = board.update_boardstate(action)\n",
        "    drawboard(board.snake, board.apple)\n",
        "\n",
        "    lensnaketext     = font.render('          LEN OF SNAKE: ' + str(len_of_snake), False, (255, 255, 255))\n",
        "    prevlensnaketext = font.render('          LEN OF PREVIOUS SNAKE: ' + str(prev_len_of_snake), False, (255, 255, 255))\n",
        "\n",
        "    x_pos= (int)(0.75*windowwidth)\n",
        "    win.blit(lensnaketext, (x_pos, 40))\n",
        "    win.blit(prevlensnaketext, (x_pos, 80))\n",
        "\n",
        "    VIDEO.append(pygame.image.tostring(win, 'RGB', False))\n",
        "\n",
        "    for event in pygame.event.get():\n",
        "        if event.type==pygame.KEYDOWN and event.key == pygame.K_ESCAPE:\n",
        "            runGame = False\n",
        "\n",
        "    keys = pygame.key.get_pressed()\n",
        "    if keys[pygame.K_r]:\n",
        "        paused = True\n",
        "        while paused == True:\n",
        "            clock.tick(10)\n",
        "            pygame.event.pump()\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == pygame.KEYDOWN:\n",
        "                    paused = False\n",
        "\n",
        "    pygame.display.update()\n",
        "\n",
        "    if board.game_over == True:\n",
        "        prev_len_of_snake = len_of_snake\n",
        "        board.resetgame()\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MPV4')\n",
        "output_name = 'output_' + snake_name + '.mp4'\n",
        "video_mp4 = cv2.VideoWriter(output_name,fourcc, 20.0, (windowwidth,windowheight))\n",
        "\n",
        "for image in VIDEO:\n",
        "\n",
        "    image = np.frombuffer(image, np.uint8).reshape(windowheight, windowwidth, 3)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)\n",
        "    video_mp4.write(image)\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "video_mp4.release()\n",
        "\n",
        "pygame.quit()"
      ],
      "metadata": {
        "id": "gJlqNcku7voZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}